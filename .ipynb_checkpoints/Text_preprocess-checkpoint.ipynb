{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611ddec4",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77eaf18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7736cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            lines.append(line)\n",
    "            line = f.readline()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fc743bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/mannpurohit/Spark/Learning/data/text_data.txt'\n",
    "lines = read_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ca519",
   "metadata": {},
   "source": [
    "### 1) check to see if a row only contains whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b520c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_blanks(text):\n",
    "#     is_blank = str(data_str.isspace())\n",
    "    text = text.strip()\n",
    "    if len(text) == 0:\n",
    "        return 'False'\n",
    "    else:\n",
    "        return 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8831e496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_blanks(lines[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ae91c",
   "metadata": {},
   "source": [
    "### 2) Determine whether the language of the text content is english or not: Use langid module to classify the language to make sure we are applying the correct cleanup actions for English langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1faf863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langid\n",
    "# import langid\n",
    "# def check_lang(data_str):\n",
    "#     predict_lang = langid.classify(data_str)\n",
    "#     if predict_lang[1] >= .9:\n",
    "#         language = predict_lang[0]\n",
    "#     else:\n",
    "#         language = 'NA'\n",
    "#     return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67126a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in lines:\n",
    "#     print(check_lang(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4fa3d",
   "metadata": {},
   "source": [
    "### 3) Removing punctuations, blank spaces, and other unnecessary strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c84fcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Removing URLs\n",
    "    url_re = re.compile('(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    text = url_re.sub(' ', text)\n",
    "    \n",
    "    # Removing all the words containing numbers and lower casing the text\n",
    "    pattern = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "    output = ' '.join(pattern.findall(text)).lower()\n",
    "    \n",
    "    # Removing single letters like a, i, u\n",
    "    output = re.sub(r'\\b[a-z]\\b', '', output)\n",
    "    \n",
    "    # Removing blank spaces\n",
    "    output = re.sub(r'\\s+', ' ', output).strip()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e95a15",
   "metadata": {},
   "source": [
    "## 4) Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b128d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stops(data_str):\n",
    "    # expects a string\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        if word not in stops:\n",
    "            # rebuild cleaned_str\n",
    "            if list_pos == 0:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915ea17",
   "metadata": {},
   "source": [
    "## 5) Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a061e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk import word_tokenize\n",
    "def lemmatize(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    pp_list = []\n",
    "    for word in words:\n",
    "        pp_list.append(stemmer.stem(word))\n",
    "    \n",
    "    pp_text = ' '.join(pp_list)\n",
    "    return pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392f7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
